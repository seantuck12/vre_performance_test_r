---
title: "R performance benchmarking"
subtitle: "For purpose of comparing IRC and LASER VREs"
author: Sean Tuck
date: 2020-07-13
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Requirement

A requirement of replacing LIDA's IRC secure platform with the new Microsoft Azure cloud platform, LASER, is that research code run at least as quickly on LASER VREs as it does on IRC.

Comparing VREs may be problematic, because of differences in hardware. However, that aside, we can do some **basic benchmarking to ensure that a standard spec IRC VRE performs no faster than a standard spec LASER VRE**.

It has been requested that benchmarking be done using programming languages commonly used for research code, so that it broadly reflects user experience. Hence this benchmarking performed with R. Similar testing will also be done using SQL code, and possibly Python too.

## Benchmarking report

This markdown document serves as a benchmarking report that can be done on any (virtual) machine with R installed. The report should include some basic information about the hardware that's used to run it, and one or several run times of computationally intensive operations. Each report can be saved and the logged run times observed to compare as many machines as is necessary.

The benchmarking will run matrix calculations and multithreaded operations.

### System information

```{r}
Sys.info()
```

Use `nodename` and time to create a test ID that can be referred to in later use.

```{r}
print(test_id)
```

Note: Code to establish hardware specs assumes Windows OS (IRC and LASER both Windows VREs - by default).

### CPU

```{r}
cpu <- system('wmic cpu get /VALUE', intern = TRUE)
cpu <- cpu[grepl("(^Name|^MaxClockSpeed|^NumberOfCores|^NumberOfLogicalProcessors)", cpu)]
cat(paste(cpu, collapse = "\n"))
```

### RAM

```{r}
ram <- system("wmic MemoryChip get Capacity", intern = TRUE)
ram <- sum(as.numeric(ram), na.rm = TRUE)
sprintf("RAM capacity: %.1f GB", ram / (1024^3))
```

### RStudio and R info

```{r}
data.frame(.Platform)
```

```{r}
R.version
```

Full details of current R session:

```{r}
sessionInfo()
```

## Performance benchmarking

Define number of runs each computation will be iterated over.

```{r}
num_runs <- 3
```

Create matrix: 

```{r}
n <- 2000
X <- rWishart(1, n, diag(n))[ , ,1]
Y <- rWishart(1, n, diag(n))[ , ,1]
```

Compute the matrix crossproduct X*'Y:

```{r}
(crossprod_time <- system.time(for (i in 1:num_runs) crossprod(X, t(Y))))
```

Compute the Cholesky decomposition:

```{r}
(chol_time <- system.time(for (i in 1:num_runs) chol(X)))
```

Compute the eigenvalues and eigenvectors:

```{r}
(eigen_time <- system.time(for (i in 1:num_runs) eigen(X)))
```

Perform k-means clustering on the matrix:

```{r}
(kmeans_time <- system.time(for (i in 1:num_runs) kmeans(X, centers = 10)))
```

Output the above data to CSV file to enable more direct comparisons, test-by-test, among machines.

```{r}
res <- rbind(
    data.frame(test_id = test_id, test = "crossprod", total_elapsed = crossprod_time["elapsed"]),
    data.frame(test_id = test_id, test = "chol", total_elapsed = chol_time["elapsed"]),
    data.frame(test_id = test_id, test = "eigen", total_elapsed = eigen_time["elapsed"]),
    data.frame(test_id = test_id, test = "kmeans", total_elapsed = kmeans_time["elapsed"])
  )
row.names(res) <- NULL
res$avg_elapsed <- res$total_elapsed / num_runs
res
```

```{r}
fname <- paste0("vre_performance_benchmark_results_", test_id, ".csv")
write.csv(res, paste0(output_dir, fname))
```

### Overall benchmark score

Average seconds elapsed across all iterations of all benchmarking tests:

```{r}
sprintf("OVERALL BENCHMARK SCORE (secs): %.6f", mean(res$avg_elapsed))
```

